---
title: "Documentation of Logit-Models"
author: "EZD Team"
date: "20 May 2019"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    df_print: paged
    code_folding: hide
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.width = 14, dpi = 300)

```
# Loading packages and Data required for modeling
- Loading packages
```{r eval=FALSE}
source("loadpackages.R")
```

```{r eval=FALSE}

```


- Loading the predictor rasterstack
```{r eval=FALSE}
predictors <- stack(c(
  "../../Daten/Raster/height_raster.grd",
  "../../Daten/Raster/temp_raster.grd",
  "../../Daten/Raster/rain_raster.grd",
  "../../Daten/Raster/distance_water_raster.grd",
  "../../Daten/Raster/distance_loess_raster.grd",
  "../../Daten/Raster/frostdays_raster.grd",
  "../../Daten/Raster/sunhours_raster.grd",
  "../../Daten/Raster/corine_raster.grd",
  "../../Daten/Raster/tpi_raster.grd",
  "../../Daten/Raster/slope_raster.grd",
  "../../Daten/Raster/aspect_raster.grd"))
```


- Transforming the rasterstack to units that are easily digested
```{r eval=FALSE}
predictors <- stack(c(
  predictors[[1]]/100,
  predictors[[2]],
  predictors[[3]],
  predictors[[4]]/1000,
  predictors[[5]]/1000,
  predictors[[6]],
  predictors[[7]],
  predictors[[8]],
  predictors[[9]],
  predictors[[10]],
  predictors[[11]]
))
```

- Plotting predictors to verify everything went as planned
```{r eval=FALSE}
plot(predictors)
```

# Preparation of Datasets
- Loading Data into the environment
```{r eval=FALSE}
data_sites <- readRDS("../../Daten/GWGLM/df_all.RDS")
data_settle <- readRDS("../../Daten/GWGLM/df_siedlung.RDS")
data_viereck <- readRDS("../../Daten/GWGLM/df_viereckschanze.RDS")
data_nonsites <- read.table(file = "../../Daten/neg29kfiltered.csv")
```
These sets yet contain unequal amounts of site, and nonsite points
We fix that in the following steps:
- Filtering sites from nonsites
```{r eval=FALSE}
data_sites <- filter(data_sites, site == 1)
data_settle <- filter(data_settle, site == 1)
data_viereck <- filter(data_viereck, site == 1)
```

- Automating extraction of raster data for given coordinates
```{r eval=FALSE}
#' Generate data.frame that is almost ready for modeling.
#'
#' @param sitesdata Data.frame containing site coordinates.
#' @param nonsitesdata Data.Frame containing nonsite coordinates.
#' @param predictorstack Rasterstack to extract variables from.
#'
#' @return Data.Frame that is almost ready for modeling process.
#' @export
#'
#' @examples
generateEvidence <- function(sitesdata, nonsitesdata = data_nonsites, predictorstack = predictors) {
  # selecting site points
  sites_temp <- sitesdata
  sites_temp$lon <- as.numeric(as.vector(sites_temp$lon))
  sites_temp$lat <- as.numeric(as.vector(sites_temp$lat))
  # convert to spatial data in order to extract the predictor values for all points
  coordinates(sites_temp) <- c("lon","lat")
  proj4string(sites_temp) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
  nonsites_temp <- nonsitesdata
  coordinates(nonsites_temp) <- c("lon", "lat")
  proj4string(nonsites_temp) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
  # extracting predictor values for sites and nonsites
  sSP <- SpatialPoints(sites_temp@coords)
  nsSP <- SpatialPoints(nonsites_temp@coords)
  values_sites <- extract(predictorstack, sSP)
  values_nonsites <- extract(predictorstack, nsSP)
  # converting back to data.frame for modeling
  coords_sites <- sites_temp@coords
  coords_sites <- as.data.frame(coords_sites)
  coords_nonsites <- nonsites_temp@coords
  coords_nonsites <- as.data.frame(coords_nonsites)
  
  values_sites <- as.data.frame(values_sites)
  values_nonsites <- as.data.frame(values_nonsites)
  values_sites$site <- 1
  values_nonsites$site <- 0
  values_sites$lon <- coords_sites$lon
  values_sites$lat <- coords_sites$lat
  values_nonsites$lon <- coords_nonsites$lon
  values_nonsites$lat <- coords_nonsites$lat
  # selecting unique settlementpoints
  # filtering out coordinate pairs that are equal
  # based on productindex. The Idea being that small differences in coordinate values maka a large difference in 
  # product results. And distinct pairs that yield the same result in products like (9.2, 49.1) and (49.1, 9.2)
  # are impossible to find in the dataset due to the geographic extent of bavaria. 
  # we also tried generating an index with sums which lead to the exact same amount of unique coordinate pairs.
  values_sites$index <- values_sites$lon * values_sites$lat
  values_sites <- values_sites[!duplicated(values_sites$index), ]
  values_sites$index <- NULL
  # combining everything together to a single data.frame for modeling
  evidence <- rbind(values_sites, values_nonsites)
  evidence <- na.omit(evidence)
  return(evidence)
}
```


- Generating evidence data for all three data sets
```{r eval=FALSE}
evidence_sites <- generateEvidence(sitesdata = data_sites)
evidence_settle <- generateEvidence(sitesdata = data_settle)
evidence_viereck <- generateEvidence(sitesdata = data_viereck)
```


- Automate building of sets with equal counts of site == 1 and site == 0
```{r eval=FALSE}
#' Finalize data for modeling process
#'
#' @param evd Data.frame to process. Containing site and nonsite data in unequal amounts.
#'
#' @return Data.frame containing site and nonsite data in equal amounts.
#' @export
#'
#' @examples
finalizeEvidence <- function(evd){
  siedl_pts <- filter(evd, site == 1)
  nons_pts <- filter(evd, site == 0)
  sz <- nrow(siedl_pts)
  nons_pts_sub <- sample_n(nons_pts, size = sz)
  temp <- rbind(siedl_pts, nons_pts_sub)
  return(temp)
}
```


- Finalizing datasets
```{r eval=FALSE}
set.seed(12345)
evidence_sites <- finalizeEvidence(evd = evidence_sites)
evidence_settle <- finalizeEvidence(evd = evidence_settle)
evidence_viereck <- finalizeEvidence(evd = evidence_viereck)
```


- Checking if everything went as planned
```{r eval=FALSE}
nrow(evidence_sites)
nrow(evidence_settle)
nrow(evidence_viereck)
```


# Fitting models
We fit a full model first to see if the model assumptions of logistic regression hold or if variable transformation or removal is needed.
```{r eval=FALSE}
fit_sites <- glm(site ~ height + temp + rain + distance_water + distance_loess + frostdays + slope + 
                     sunhours + corine + tpi + aspect, 
                   family = binomial(), 
                   data = evidence_sites)
summary_sites <- summary(fit_sites)
summary_sites
```

- Checking if model assumptions hold
```{r eval=FALSE}
probabilities <- predict(fit_sites, type = "response")
predicted_classes <- ifelse(probabilities > 0.5, "pos", "neg")
```


## Logistic regression diagnostics:
- Linearity of predictor variables and logit of outcome
```{r eval=FALSE}
mydata <- evidence_sites %>% 
  dplyr::select_if(is.numeric)
predictors_mydata <- colnames(mydata)
# bind logit and tidy data for plotting
mydata <- mydata %>% 
  mutate(logit = log(probabilities/(1-probabilities))) %>% 
  gather(key = "predictors.mydata", value = "predictor.value", -logit , -lon, -lat, -site)

# creating scatter plots: 
linearityplots <- ggplot(mydata, aes(logit, predictor.value)) +
  geom_point(size = 0.5, alpha = 0.5) + 
  facet_wrap(~predictors.mydata, scales = "free_y") + 
  ylab("Predictor Wert") +
  xlab("Logits") 
  
linearityplots

# Aspect and Corine code are categorical variables.
# As the summary of a model that introduces aspect as a factor of 8 levels shows,
# the variables are no longer of significance thus we decided to drop
# aspect, sunhours and corine code from inputvariables
# Moreover we decided to remove tpi because of lacking linearity
```

- Introducing Sunhours as a factor variable
```{r eval=FALSE}
predictors_factors <- predictors
predictors_factors[[7]] <- round(predictors_factors[[7]])

factor_evidence <- generateEvidence(data_sites, nonsitesdata = data_nonsites, predictorstack = predictors_factors)

fit_sites_factor <- glm(site ~ height + temp + rain + distance_water + distance_loess + frostdays + slope + 
                          height * rain + as.factor(layer), 
                        family = binomial(), 
                        data = factor_evidence)
summary_sites_factor <- summary(fit_sites_factor)
summary_sites_factor
```
As significance levels of above 0.9 indicate, the effects of sunhours are pretty much random. Thus we decided to drop sunhours from the model.

- Introducing an interaction term
```{r eval=FALSE}
fit_sites_nocat <- glm(site ~ height + temp + rain + distance_water + distance_loess + frostdays + slope + 
                         height * rain, 
                       family = binomial(), 
                       data = evidence_sites)
summary_sites_nocat <- summary(fit_sites_nocat)
summary_sites_nocat

```
Both the interaction term itself, and the variable height are now of significance.

- Checking other interactions
```{r eval=FALSE}
fit_sites_interact <- glm(site ~ height + temp + rain + distance_water + distance_loess + frostdays + slope + 
                            temp*rain + temp*frostdays + temp*rain*frostdays + rain*frostdays, 
                          family = binomial(), 
                          data = evidence_sites)
summary(fit_sites_interact)
```
This seems to reduce both interpretability of the resulting model and significance of the other variables.
Given that our goal was to produce predictive maps the interpretability of the models may not be of utmost importance, but the predictive performance of the model as measured by the AUROC only improved by 0.005 for this particular model as compared to the model containing just one interaction term. For these reasons, and a lack of distinct domain knowledge of potential interactions between archaeological and climate data, we decided not to introduce more interaction terms to our models.

## Checking model assumptions
- Confusion matrix to assess performance for a naive 0.5 classifier
```{r eval=FALSE}
probabilities <- predict(fit_sites_nocat, type = "response")
predicted_classes <- ifelse(probabilities > 0.5, "pos", "neg")
pred_cl <-  predicted_classes
reference <- ifelse(evidence_sites$site == TRUE, "pos", "neg")
pred_cl <- as.factor(pred_cl)
reference <- as.factor(reference)
# confusion matrix 
caret::confusionMatrix(pred_cl, reference)
```
- Binned residual Plot 
```{r eval=FALSE}
arm::binnedplot(fitted(fit_sites_nocat), 
           residuals(fit_sites_nocat, type = "response"), 
           nclass = 200, 
           xlab = "Expected Values", 
           ylab = "Average Residual", 
           main = "Binned Residual Plot", 
           cex.pts = 0.8, 
           col.pts = 1, 
           col.int = "gray")
```
This looks promising. As a large majority of binned residuals fall into the +-2 Standard deviation confidence band. 


- Checking linearity assumption
```{r eval=FALSE}
mydata <- evidence_sites %>% 
  dplyr::select_if(is.numeric)
predictors_mydata <- colnames(mydata)
# bind logit and tidy data for plotting
mydata <- mydata %>% 
  mutate(logit = log(probabilities/(1-probabilities))) %>% 
  gather(key = "predictors.mydata", value = "predictor.value", -logit , -lon, -lat, -site, -aspect, 
         -corine, -sunhours, -tpi)

# creating scatter plots: 
linearityplots_nocat <- ggplot(mydata, aes(logit, predictor.value)) +
  geom_point(size = 0.5, alpha = 0.5) + 
  # geom_smooth(method = "loess") +
  theme_bw() +
  facet_wrap(~predictors.mydata, scales = "free_y") + 
  ylab("Predictor Wert") +
  xlab("Logits") 
  
linearityplots_nocat
```


Distance water and distance loess might be better represented through smooth terms. Fitting an additive model in the next step.

- Fitting additive model
```{r eval=FALSE}
fit_sites_gam <- gam(site ~ height + temp + rain + frostdays + slope + 
                       height * rain + s(distance_water) + s(distance_loess),
                     family = binomial(),
                     data = evidence_sites)
summary(fit_sites_gam)
plot.gam(fit_sites_gam)
```
The strength of the effects do not seem to be influenced by the distances themselves. Especially distance_water hovers around 0. 

- Analysis of Influential
```{r eval=FALSE}
cookplot <- plot(fit_sites_nocat, which = 4, id.n = 10)
mdl_data <- augment(fit_sites_nocat) %>% 
  mutate(index = 1:n())
# displaying top 10 largest values
topnoutliers <- mdl_data %>% top_n(10, .cooksd)
# plotting standardized residuals
stdresplot <- ggplot(mdl_data, aes(index, .std.resid)) +
  geom_point(aes(color = site), alpha = 0.5) +
  scale_y_continuous(name="Standardized Residuals", limits=c(-3, 5)) +
  theme_bw()
# filtering potential influential data points 
infpoints <- mdl_data %>% 
  filter(abs(.std.resid) > 3)
```
There seem to be 7 influential points. 

- Examination of Multicolinearity
```{r eval=FALSE}
vif <- car::vif(fit_sites_nocat)
vif
```
The Variance inflation factors of height, rain and the interaction term is expected to be high because of that very interaction term we introduced into the model. The other VIFs are smaller than 5 across the board. Everything looks good. 

- Assessing overall classifier performance through AUROC
```{r eval=FALSE}
pROC::auc(pROC::roc(evidence_sites$site, fitted(fit_sites_nocat)))
pROC::auc(pROC::roc(evidence_sites$site,fitted(fit_sites_interact)))
```
More beautiful ROC-Plots can be generated with Deducer::rocplot(). The Package Deducer requires rJava to be installed and thus also requires JAVA 11 SKD to be installed on the system. Aditionally it requires a call of Sys.setenv() to set the correct path to java like so: 
```{r eval=FALSE}
Sys.setenv(JAVA_HOME="C:/PATH/TO/JAVA/")
# Generate a nicer ROC-plot
library(Deducer)
Deducer::rocplot(fit_sites_nocat)
```


# Predictive Mapping
- Predictive Map for the full model
```{r eval=FALSE}
env_data_df_full <- as.data.frame(predictors)
pdata <- predict(fit_sites_nocat, newdata = env_data_df_full, type = "response")

x_pred <- predictors
x_pred$pred <- pdata

# predictive plot for linear model
plot(x_pred$pred)
```

- Fitting models for settlements and Viereckschanzen
```{r eval=FALSE}
fit_settle <- glm(site ~ height + temp + rain + distance_water + distance_loess + frostdays + slope + 
                    height * rain, 
                  family = binomial(), 
                  data = evidence_settle)
summary_settle <- summary(fit_settle)
summary_settle

pROC::auc(pROC::roc(evidence_settle$site, fitted(fit_settle)))

arm::binnedplot(fitted(fit_settle), 
                residuals(fit_settle, type = "response"), 
                nclass = 200, 
                xlab = "Expected Values", 
                ylab = "Average Residual", 
                main = "Binned Residual Plot", 
                cex.pts = 0.8, 
                col.pts = 1, 
                col.int = "gray")

fit_viereck <- glm(site ~ height + temp + rain + distance_water + distance_loess + frostdays + slope + 
                     height * rain, 
                 family = binomial(), 
                 data = evidence_viereck)
summary_viereck <- summary(fit_viereck)
summary_viereck

pROC::auc(pROC::roc(evidence_viereck$site, fitted(fit_viereck)))

arm::binnedplot(fitted(fit_viereck), 
                residuals(fit_viereck, type = "response"), 
                nclass = 50, 
                xlab = "Expected Values", 
                ylab = "Average Residual", 
                main = "Binned Residual Plot", 
                cex.pts = 0.8, 
                col.pts = 1, 
                col.int = "gray")
```

- Assessing Stability of Model Estimators
For each model we have only one set of data points, but through the sampling process of negative datapoints we do have a large selection of nonsite points to choose from. It only seemed reasonable to inspect the stability of the estimated model coefficients through resampling of nonsite datapoints. 
```{r eval=FALSE}
# Start off by making a copy of data_sites for further processing
stability_sites <- data_sites
# Generating evidence like usual
evidence_stability <- generateEvidence(sitesdata = stability_sites)
# Create an empty data.frame to save the estimated model coefficients into
df_stability <- data.frame(matrix(nrow = 500,ncol = length(fit_sites_nocat$coefficients)))
colnames(df_stability) <- names(fit_sites_nocat$coefficients)
# The following loop redraws a new set of negative datapoints in every iteration.
# Then a model mdl gets estimated based on the new dataset.
# The estimated coefficients get saved as the ith row in our data.frame.
# We then print i because my laptop is slow and I want to see the progress.
for (i in 1:nrow(df_stability)) {
  temp <- finalizeEvidence(evidence_stability)
  mdl <- glm(site ~ height + temp + rain + distance_water + distance_loess + frostdays + slope + 
               height * rain, 
             family = binomial(), 
             data = temp)
  df_stability[i,] <- mdl$coefficients
  print(i)
}
head(df_stability)
# We are not interested in the intercept, to keep the boxplots clean we will drop 
# it from the data.frame
df_stability <- df_stability[,2:9]
colnames(df_stability) <- c("height", "temp", "rain", "wdist", "ldist", "frost", "slope", "HR-inter")
require(reshape2)
ggplot(data = melt(df_stability), aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=variable)) + 
  ylab("Coefficient Estimates") +
  xlab("") + 
  theme(legend.position = "none")
```


- visualizing of interaction term
```{r eval=FALSE}
interdata <- evidence_sites
interdata$height_bin <- ifelse(interdata$height > mean(interdata$height), "above avg","below avg")
interdata$rain_bin <- ifelse(interdata$rain > mean(interdata$rain), "above avg","below avg")
interdata$prob <- fitted(fit_sites_nocat)
interdata$type_num <- interdata$site

# plotting interaction
ggplot(interdata, aes(rain, type_num)) +
  geom_point() +
  stat_smooth(aes(rain, prob, col = height_bin), se = F) +
  labs(y = "Pr(site)", title = "site ~ rain varying by height_bin")

ggplot(interdata, aes(height, type_num)) +
  geom_point() +
  stat_smooth(aes(height, prob, col = rain_bin), se = F) +
  labs(y = "Pr(site)", title = "Diabetes ~ height varying by rain")
```

# Methods of Crossvalidation
To estimate the general performance of the classifier we will carry out two different methods of repeated cross validation. 
- Setting up the data, learners, and tasks
```{r eval=FALSE}
coords = evidence_sites[, c("lon", "lat")]
# Select response and predictors to use in the modeling
data = dplyr::select(evidence_sites, -lon, -lat)
# Create task
data$site <- as.logical(data$site)
task = makeClassifTask(data = data, target = "site",
                       positive = "TRUE", coordinates = coords)

# Specify a learner
lrn = makeLearner(cl = "classif.binomial",
                  link = "logit",
                  predict.type = "prob",
                  fix.factors.prediction = TRUE)
# specifying a resampling method (spRepCV)
perf_level = makeResampleDesc(method = "SpRepCV", folds = 5, reps = 100)
```
- Running resampling
```{r eval=FALSE}
set.seed(37)
sp_cv = mlr::resample(learner = lrn, task = task,
                      resampling = perf_level, 
                      measures = mlr::auc)
boxplot(sp_cv$measures.test$auc)

perf_level_std <- makeResampleDesc(method = "RepCV", folds = 5, reps = 100)
std_cv <- mlr::resample(learner = lrn, task = task,
                        resampling = perf_level_std,
                        measures = mlr::auc)
```

- Visualize results of both resampling methods
```{r eval=FALSE}
boxplot(std_cv$measures.test$auc, sp_cv$measures.test$auc)
```


- Visualize the resampling 
```{r eval=FALSE}
resplots <- createSpatialResamplingPlots(task = task, resample = std_cv, crs = "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0", repetitions = 1,
                                         x.axis.breaks = c(8.5, 14.5),
                                         y.axis.breaks = c(47, 51))
resplots[[1]][[1]]
# classic cv 
rdesc = makeResampleDesc("RepCV", folds = 5, reps = 4)
resamp = resample(makeLearner("classif.binomial"), task = task, rdesc)
##
plots.classic = createSpatialResamplingPlots(task = task, resamp, crs = "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0",
                                             repetitions = 2, x.axis.breaks = c(8.5, 14.5),
                                             y.axis.breaks = c(47, 51))
plots.classic[[1]][[3]]
plots.classic[[1]][[1]]
# plotting them all in a grid: 
require("cowplot")
cowplot::plot_grid(plotlist = plots[["Plots"]], ncol = 3, nrow = 1,
                   labels = plots[["Labels"]])
```




